# EvalEval

This repository contains the code for the paper [Perturbation Checklist for Evaluating NLG Evaluation Metrics]()

## Contents

- [Overview](#overview)
- [Templates](#templates)
  - [Data-to-Text Generation](#data-to-text-generation)
  - [Image Captioning](#image-captioning)
  - [Translation](#translation)
  - [Dialogue](#dialogue)
  - [Summarization](#Summarization)
  - [Question Generation](#question-generation)
- [Human Evaluations](#human-evaluations)
- [Metrics](#metircs)
- [Citation](#citation)

## Overview

In this work we provide a detailed analysis of NLG metrics by going beyond correlation with human scores. We propose a comprehensive criteria-checklist based evaluation that will act as a diagnostic tool in pointing out specific avenues of improvement in metrics. We create specific [templates](#templates) that are targeted to test the performance of a metric along a particular dimension. <br>

Please find more details of this work in our [paper]().

## Templates

All the templates used in our works have been made available in the `templates/` folder and are categorized in the following sections <br>

Each of the NLG tasks have the following criteria, the table can also be found in our paper and has been referred from the survey paper, [A Survey of Evaluation Metrics Used for NLG Systems](https://arxiv.org/abs/2008.12009)

| Task| Criteria |
| -----| ------| 
| Machine Translation | Fluency, Adequacy |
| Abstrative Summarization | Fluency , Coherence , Relevance, Coverage, Clarity |
| Image Captioning | Fluency, Thoroughness , Correctness |
| Data to Text Generation | Fluency ,Correctness, Coverage , Relevance |
| Question Generation | Fluency , Answerability, Relevance |
| Dialogue | Fluency, Relevance, Making sense, Interesting, Avoid Repetition |

<br> 



## Human Evaluations

The human annotations collected for the templates can be downloaded from [here](#gdrive-link)

## Metrics

Coming soon ..

## Citation

If you find our work useful please cite
```

```
